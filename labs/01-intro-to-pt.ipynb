{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Course - LAB 1\n",
    "\n",
    "## Intro to PyTorch\n",
    "\n",
    "PyTorch (PT) is a Python (and C++ and Java) library for Machine Learning (ML) particularly suited for Neural Networks and their applications.\n",
    "\n",
    "Its great selection of built-in modules, models, functions, CUDA capability, tensor arithmetic support and automatic differentiation functionality make it one of the most used scientific libraries for Deep Learning.\n",
    "\n",
    "Note: for this series of labs, we advise to install Python >= 3.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing PyTorch\n",
    "\n",
    "We advise to install PyTorch following the directions given in its [home page](https://pytorch.org/get-started/locally/). Just typing `pip install torch` may not be the correct action as you have to take into account the compatibility with `cuda`. If you have `cuda` installed, you can find your version by typing `nvcc --version` in a terminal (Linux/iOS). We suggest installing PyTorch inside a Conda environment as for the link indicated before.\n",
    "\n",
    "If you're using Windows, we first suggest first to install Anaconda and then install PyTorch from the `anaconda prompt` software via `conda` (preferably) or `pip`.\n",
    "\n",
    "If you're using Google Colab, all the libraries needed to follow this lecture should be pre-installed there.\n",
    "\n",
    "We see now how to operate on Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Colab users\n",
    "\n",
    "Google Colab is a handy tool that we suggest you use for this course---especially if your laptop does not support CUDA or has limited hardware capabilities. Anyway, note that **we'll try to avoid GPU code as much as possible**.\n",
    "\n",
    "Essentially, Colab renders available to you a virtual machine with a limited hardware capability and disk where you can execute your code inside a given time window. You can even ask for a GPU (if you use it too much you'll need to start waiting a lot before it's available though)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your (maybe) first Colab commands\n",
    "\n",
    "Colab Jupyter-style notebook interface with a few tweaks.\n",
    "\n",
    "For instance, you may run (some) bash command from here prepending `!` to your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/mnzluca/IntroToAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes it very easy to operate your virtual machine without the need for a terminal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File transfer on Colab\n",
    "\n",
    "One of the most intricate action in Colab is file transfer. Since your files reside on the virtual machine, there're two main ways to operate file transfer on Colab.\n",
    "\n",
    "* `files.download()` / `.upload()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(\"sample_data/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it may be much more handy to connect your Google Drive to Colab. Here is a snippet that lets you do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "folder_mount = '/content/drive' # Your Drive will be mounted on top of this path\n",
    "\n",
    "drive.mount(folder_mount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For VsCode users\n",
    "\n",
    "Visual Studio Code works perfectly if you're using conda environments.\n",
    "\n",
    "Just create your own conda environment, then, on VsCode, hit `Ctrl + Shift + p` and type `Python: select interpreter`. The dropdown menu should list the Python binaries within the envs you have created on your machine.\n",
    "\n",
    "![](imgs/01/vscode_select_interpreter.jpg)\n",
    "\n",
    "![](imgs/01/vscode_list_envs.jpg)\n",
    "\n",
    "Moreover, is you're running a Jupyter Notebook, you may select an interpreter the first time you run a cell during the current session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dive into PyTorch - connections with NumPy\n",
    "\n",
    "Like NumPy, PyTorch provides its own multidimensional array class, called `Tensor`. `Tensor`s are essentially the equivalent of NumPy `ndarray`s.\n",
    "If we wish to operate a very superficial comparison between `Tensor` and `ndarray`, we can say that:\n",
    "* `Tensor` draws a lot of methods from NumPy, although it's missing some. The gap though is getting smaller and smaller each new release of PyTorch\n",
    "* `Tensor` is more object oriented than `ndarray` and solves some inconsistencies within NumPy\n",
    "* `Tensor` has CUDA support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# create custom Tensor and ndarray\n",
    "x = torch.Tensor([[1,5,4],[3,2,1]])\n",
    "y = np.array([[1,5,4],[3,2,1]])\n",
    "\n",
    "def pretty_print(obj, title=None):\n",
    "    if title is not None:\n",
    "        print(title)\n",
    "    print(obj)\n",
    "    print(\"\\n\")\n",
    "\n",
    "pretty_print(x, \"x\")\n",
    "pretty_print(y, \"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the types of these objs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.dtype, y.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch` already thinks with Machine Learning in mind as the `Tensor` is implicitly converted to `dtype float32`, while NumPy makes no such assumption.\n",
    "\n",
    "For more info on `Tensor` data types, please check the beginning of [this page](https://pytorch.org/docs/stable/tensors.html). We can use `Tensor` methods such as `.half()` or `.bool()` to convert the tensor from `float32` to `float16` or `bool` respectively.\n",
    "\n",
    "As in NumPy, we can call the `.shape` attribute to get the shape of the structures. Moreover, `Tensor`s have also the `.size()` method which is analogous to `.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape, y.shape, x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how a `Tensor` shape is **not** a tuple.\n",
    "\n",
    "We can also create a random `Tensor` analogously to NumPy.\n",
    "\n",
    "A `2 × 3 × 3` `Tensor` is the same as saying \"2 3 × 3 matrices\", or a \"cubic matrix\"\n",
    "\n",
    "![](img/tensors.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand([2, 3, 3])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.rand(2, 3, 3)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the total number of elements in a `Tensor` via the `numel()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the memory occupied by each element of a `Tensor` via `element_size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.element_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can quickly calculate the size of the `Tensor` within the RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.numel() * x.element_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slicing a `Tensor`\n",
    "\n",
    "You can slice a `Tensor` (*i.e.*, extract a substructure of a `Tensor`) as in NumPy using the square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract first element (i.e., matrix) of first dimension\n",
    "pretty_print(x[0], \"Slice first element (x[0])\")\n",
    "\n",
    "# extract a specific element\n",
    "pretty_print(x[1,2,0], \"Slice element at (1, 2, 0) (x[1, 2, 0])\")\n",
    "\n",
    "# extract first element of second dimension (\":\" means all the elements of the given dim)\n",
    "pretty_print(x[:, 0], \"Slice first element of second dim (x[:, 0])\")\n",
    "\n",
    "# note that it is equivalent to\n",
    "pretty_print(x[:, 0, :], \"As above (x[:, 0] equivalent to x[:, 0, :])\")\n",
    "\n",
    "# extract range of dimensions (first and second element of third dim) \n",
    "pretty_print(x[:, :, 0:2], \"Slice first and second el of third dim (x[:, :, 0:2])\")\n",
    "\n",
    "# note that it is equivalent to (i.e., you can also pass list for slicing, as opposed to Py vanilla lists/tuples)\n",
    "pretty_print(x[:, :, (0, 1)], \"As above (x[:, :, 0:2] equivalent to x[:, :, (0, 1)])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Py, you can also slice any list by interval via the \"double colon\" notation `::` (`from` : `(to - 1)` :`step`). Note that `::3` means \"take all elements of the object by step of 3 starting from 0 until the list ends\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange(0, 10)[0:7:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Tensor` supports linear algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = torch.rand([4, 5])\n",
    "print(\"z1\")\n",
    "print(\"shape\", z1.shape)\n",
    "print(z1)\n",
    "\n",
    "# transposition\n",
    "z2 = z1.T\n",
    "\n",
    "print(\"\\nz2\")\n",
    "print(\"shape\", z2.shape)\n",
    "print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication\n",
    "pretty_print(z1 @ z2, \"Matrix multiplication: with '@'\")\n",
    "\n",
    "# equivalent to\n",
    "pretty_print(torch.matmul(z1, z2), \"Matrix multiplication: with torch.matmul\")\n",
    "\n",
    "# and also\n",
    "pretty_print(z1.matmul(z2), \"Matrix multiplication: with Tensor.matmul\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `@` identifies the matrix product.\n",
    "\n",
    "Don't mistake `@` and `*` as the latter is the Hadamard (element-by-element) product!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: is it possible to do scalar product between vectors?\n",
    "\n",
    "Note: the scalar product is also called [inner or dot or projection product](https://en.wikipedia.org/wiki/Dot_product).\n",
    "\n",
    "Like we have seen with matrices, we can proceed as follows for vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec1 = torch.rand((5,))\n",
    "vec2 = torch.rand((5,))\n",
    "\n",
    "pretty_print(vec1, \"vec1\")\n",
    "pretty_print(vec2, \"vec2\")\n",
    "\n",
    "pretty_print(vec1 @ vec2, \"I can use '@' even if the two vectors aren't conformable (i.e. both colum or row vectors\")\n",
    "\n",
    "pretty_print(vec1.unsqueeze(0) @ vec2.unsqueeze(-1), \"This is the 'classical' inner product between row and column vector -- Notice the dim of the output\")\n",
    "\n",
    "pretty_print(torch.matmul(vec1, vec2), \"Notice that the same result can be achieved via the `matmul` method of torch -- as we have seen, it's the inner product\")\n",
    "\n",
    "pretty_print(torch.dot(vec1, vec2), \"...and also via torch.dot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot works only with vectors!\n",
    "torch.dot(torch.rand((3,4)), torch.rand(4,3))\n",
    "\n",
    "# do this instead!\n",
    "torch.matmul(torch.rand((3,4)), torch.rand(4,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On to the tutorial, let us see more examples of operations on tensors and conformability between tensors...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 * z2 # this gives an Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 * z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, the \"regular\" arithmetic operators for Python act as element-wise operators in `Tensor`s (as in `ndarrays`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 ** 2 # Equivalent to above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z3 = torch.Tensor([[1,2,3,4,7],[0.2,2,4,5,3],[-1,3,-4,2,2],[1,1,1,1,2]])\n",
    "pretty_print(z1 % z3, \"z1 % z3 (remainder of integer division)\")\n",
    "pretty_print(z3 // z1, \"z3 // z1 (integer division)\") # integer division\n",
    "z3 /= z1\n",
    "pretty_print(z3, \"in-place tensor division (z3 /= z1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some PyTorch-based libraries, you will also see things like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z3.mul(z1)\n",
    "#this is equivalent to z3*z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or, for in-place operations, you will also see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z3.mul_(z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this also applies to other operations (not necessarily arithmetical, also boolean):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(z3 == z3 ** 2, \"In the Pythonic way (z3 == z3 ** 2)...\")\n",
    "\n",
    "pretty_print(z3.eq(z3.pow(2)), \"...and in the 'PyTorch' way (z3.eq(z3.pow(2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch abides to one of Python unwritten rules: method names ending with a single \"_\" and not starting with one are meant to indicate in-place operations, which mutate the object they're called from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for `ndarrays`, `Tensor`s arithmetic operations support **broadcasting**. Roughly speaking, when two `Tensor`s have different shapes and a binary operator is applied to them, PT will try to find a way to make these objects \"compatible\" for the operation. \n",
    "\n",
    "Of course, broadcasting is not always possible, but as a rule of thumb, if some dimensions of a `Tensor` are one and the other dimensions are the same, broadcasting works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_vector_5 = torch.Tensor([1,2,3,5,2]) # this is treated as a row vector (1 x 5 matrix)\n",
    "print(\"small_vector_5:\", small_vector_5, \"; Shape:\", small_vector_5.shape, \"\\n\")\n",
    "\n",
    "pretty_print(z1 / small_vector_5, \"Broadcasting: dividing matrix by row vector\")\n",
    "\n",
    "small_vector_4 = torch.Tensor([4,2,3,1])\n",
    "small_vector_4 = small_vector_4.unsqueeze(-1) # this operation \"transposes\" the vector into a column vector (4 x 1 matrix)\n",
    "print(\"small_vector_4:\\n\", small_vector_4, \"\\nShape:\", small_vector_4.shape, \"\\n\")\n",
    "\n",
    "pretty_print(z1 / small_vector_4, \"Broadcasting: dividing matrix by column vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Tensor([1,2,3]) == torch.Tensor([[1,2,3]]) # single-dim Tensors are also row vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshaping and permuting\n",
    "\n",
    "Sometimes it may be necessary to reshape the tensors to apply some specific operators.\n",
    "\n",
    "Take the example of RGB images: they can be seen as `3 x h x w` tensors, where `h` is the height and `w` the width.\n",
    "\n",
    "![](img/image_tensor.png)\n",
    "\n",
    "Sometimes, it may be necessary to \"flatten\" the three matrices into vectors, thus obtaining a `3 x hw` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.load(\"data/img.pt\")\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This flattening may be achieved via the `reshape` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_reshaped = image.reshape(3, 243*880)\n",
    "pretty_print(image_reshaped.shape, \"shape of image_reshaped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can alternatively use the `view` method..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_view = image.view(3, 243*880)\n",
    "pretty_print(image_view.shape, \"shape of image_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: what is the difference between `reshape` and `view`?\n",
    "\n",
    "* `view` does not copy the object. It's just a _view_ of the original tensor.\n",
    "* `reshape` sometimes copy the object, sometimes it does not and resorts to `view` \n",
    "\n",
    "The cases when the two methods behave differently are marginal and we don't have time to discuss that here (you can see for instance [this stackoverflow](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch)).\n",
    "\n",
    "As a general rule, use `reshape` instead of `view` because the latter may throw unexpected errors, while with the former you're always sure your code will run seamlessly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some libraries encode images as `h x w x 3` tensors instead of `3 x h x w`.\n",
    "\n",
    "To convert between these two format, one may be tempted to `reshape` or `view` the tensor: in the end, they share the number of elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image2 = image.reshape(243, 880, 3)\n",
    "\n",
    "plt.imshow(image2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That does not seem to work though: reshape does not change the order of the elements within the memory.\n",
    "\n",
    "In order to do so, we need to use `permute`, which changes shape **and** the order of the elements.\n",
    "We need to pass the new order of the dimensions to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image3 = image.permute(1,2,0) # 1,2,0 --> old dim1 goes first, old dim2 goes second, dim0 goes last\n",
    "# can also do image.permute(-2,-1,0) -- works with negative indices as well\n",
    "plt.imshow(image3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More linear algebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z3_norm = z3.norm()\n",
    "pretty_print(z3_norm, \"Tensor norm\")\n",
    "pretty_print(np.linalg.norm(z3), \"ndarray norm\") # notice how torch is more OO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how methods reducing `Tensor`s to scalars still return singleton `Tensor`s. (be wary of this feature when scripting something in PT)\n",
    "\n",
    "To \"disentangle\" the scalar from a `Tensor` use the `.item()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z3_norm.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, as for NumPy, PT supports `Tensor`s operator on a subset of its dimensions.\n",
    "\n",
    "For example, given a `3x4x4 Tensor`, we might want to calculate the norm of each of the three `4x4` matrices composing it. We must hence specify to the `norm` method the dimensions on which we want it to operate the reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z4 = torch.rand((3,4,5))\n",
    "pretty_print(z4, \"z4\")\n",
    "pretty_print(z4.norm(dim=(1,2)), \"Norm of the three matrices composing z4 -- z4.norm(dim=(1,2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the result is a `1x3 Tensor`, showing the norm of each of the matrices.\n",
    "\n",
    "We can notice this behaviour in other `Tensor` operator applying a reduction, for example `.sum()` and `.prod()` (sum/product of the elements within the tensor).\n",
    "\n",
    "By specifying `z4.prod(dim=1)`, we **fix** the second dimension and loop through the other dimensions, applying the product for all of the resulting tensor slices. Let's see a code for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. let us create an empty vector with the shape of the dimension we did NOT specify in `prod` above:\n",
    "product_fix_dim = torch.empty([z4.shape[0], z4.shape[2]])\n",
    "\n",
    "# 2. we loop through these dimensions to get the result of `z4.prod(dim=1)`\n",
    "for i in range(product_fix_dim.shape[0]):\n",
    "    for j in range(product_fix_dim.shape[1]):\n",
    "        # the result for the (i,j) position is the product of the corresponding sub-tensor z4[i, :, j]\n",
    "        # (we consider the whole of the 2nd dim, loop through the 1st and 3rd)\n",
    "        product_fix_dim[i, j] = z4[i, :, j].prod()\n",
    "\n",
    "pretty_print(product_fix_dim, \"The result of our fancy loop ...\")\n",
    "pretty_print(z4.prod(dim=1), \"... is the same as z4.prod(dim=1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogously, we may apply the same reasoning to the `sum()` method:\n",
    "\n",
    "`z4.sum(dim=0)`:\n",
    "* we fix the first dim\n",
    "* and loop through the other dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(z4.sum(dim=0), \"z4.sum(dim=0) -- Element-wise sum of the three matrices composing the tensor -- is a 4x5 matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is analogous to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z4[0] + z4[1] + z4[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seamless conversion from NumPy to PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_numpy = np.random.rand(3,5)\n",
    "y_torch = torch.from_numpy(y_numpy)\n",
    "pretty_print(y_torch, \"y converted to torch.Tensor\")\n",
    "\n",
    "x = torch.rand(6,4)\n",
    "x_numpy = x.numpy()\n",
    "pretty_print(x_numpy, \"x converted to numpy.ndarray\")\n",
    "\n",
    "# Note that NumPy implicitly converts Tensor to ndarray whenever it can; the same doesn't happen for PT\n",
    "pretty_print(np.linalg.norm(x), \"Example of implicit conversion Tensor → ndarray (np.linalg.norm(x) where x is torch.tensor)\")\n",
    "\n",
    "torch.norm(x_numpy) # this does not work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stochastic functionalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can render the (pseudo)random number generator deterministic by calling `torch.manual_seed(integer)`.\n",
    "\n",
    "This works for both CPU and CUDA RNG calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123456)\n",
    "print(\"...from now on our random tensor should be the same...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(torch.randperm(10), \"(randperm) Random permutation of 0:10\")\n",
    "\n",
    "pretty_print(torch.rand_like(z1), \"(rand_like) Create random vector with the same shape of z_1\")\n",
    "\n",
    "pretty_print(torch.randint(10, (3, 3)), \"(randint) Like rand, but with integers up to 10\")\n",
    "\n",
    "pretty_print(torch.normal(0, 1, (3, 3)), \"(normal) Sampling a 3x3 iid scalars from N(0,1)\")\n",
    "\n",
    "pretty_print(torch.normal(torch.Tensor([[1,2,3],[4,5,6],[0,0,0]]), torch.Tensor([[1,0.5,0.9],[0.5,1,0.1],[3,4,1]])), \"Sampling from 9 normals with different means and std into a (3x3) Tensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPUs\n",
    "\n",
    "All `Torch.Tensor` methods support GPU computation via built-in CUDA wrappers.\n",
    "\n",
    "Just transfer the involved `Tensor`s to CUDA and let the magic happen :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if cuda is available on this machine\n",
    "torch.cuda.is_available()\n",
    "\n",
    "has_cuda_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(has_cuda_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_cuda_gpu:\n",
    "    dim = 10000\n",
    "    large_cpu_matrix = torch.rand((dim, dim)) \n",
    "    large_gpu_matrix = large_cpu_matrix.to(\"cuda\") # Can also specify \"cuda:gpu_id\" if multiple GPUs\n",
    "    # alternatively, you may also call large_cpu_matrix.cuda() or large_cpu_matrix.cuda(0)\n",
    "else:\n",
    "    print(\"Sorry, this part of the notebook is inaccessible since it seems you don't have a CUDA-capable GPU on your device :/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(large_cpu_matrix.device, \"Device of large_cpu_matrix\")\n",
    "pretty_print(large_gpu_matrix.device, \"Device of large_gpu_matrix\")\n",
    "pretty_print(large_gpu_matrix, \"If a tensor is not on CPU, the device will also be printed if you print the tensor itself\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if has_cuda_gpu:\n",
    "    import timeit\n",
    "\n",
    "    # NOTE: please fix this number w.r.t. your GPU and CPU\n",
    "    repetitions = 100\n",
    "\n",
    "    print(\"Norm of large cpu matrix. Time:\", timeit.timeit(\"large_cpu_matrix.norm()\", number=repetitions, globals=locals()))\n",
    "    print(\"Norm of large gpu matrix. Time:\", timeit.timeit(\"large_gpu_matrix.norm()\", number=repetitions, globals=locals()))\n",
    "else:\n",
    "    print(\"Sorry, this part of the notebook is inaccessible since it seems you don't have a CUDA-capable GPU on your device :/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_cpu_matrix2 = torch.rand((dim, dim))\n",
    "\n",
    "\n",
    "print(\"Norm of large cpu matrix. Time:\", timeit.timeit(\"large_cpu_matrix.norm()\", number=100, globals=locals()))\n",
    "print(\"Norm of large gpu matrix. Time:\", timeit.timeit(\"large_cpu_matrix.cuda().norm()\", number=100, globals=locals()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: Why that difference between the first and the second cells?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Captain obvious: Use `tensor.cpu()` or `tensor.to(\"cpu\")` to move a tensor to your CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building easy ML models\n",
    "\n",
    "By using all the pieces we've seen till now, we can build our first ML model using PyTorch: a linear regressor, whose model is\n",
    "\n",
    "$y = XW + b$\n",
    "\n",
    "which can also be simplified as\n",
    "\n",
    "$y = XW$\n",
    "\n",
    "if we incorporate the bias $b$ inside $W$ and add to the $X$ a column of ones to the right.\n",
    "\n",
    "![](imgs/01/fakedata.jpg)\n",
    "\n",
    "We'll first create our data. The $X$ 's are the 0:9 range plus some iid white noise, while the $y$ is just the 0:9 range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.arange(0, 10).float().unsqueeze(-1)\n",
    "x2 = torch.arange(0, 10).float().unsqueeze(-1)\n",
    "x3 = torch.arange(0, 10).float().unsqueeze(-1)\n",
    "x0 = torch.ones([10]).float().unsqueeze(-1)\n",
    "# first, concatenate x1, x2, and x3 to form a single matrix X\n",
    "X = torch.cat((x1, x2, x3), dim=1)\n",
    "# add small noise to the Xs, so that we don't have a trivial \"interpolation\" problem\n",
    "eps = torch.normal(0, .3, (10, 3))\n",
    "X += eps\n",
    "# concatenate also x0 so we can express the model as y=XW\n",
    "X = torch.cat((X, x0), dim=1)\n",
    "\n",
    "pretty_print(X, \"X (covariates)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: how can we obtain the same `X` but first concatenating, then summing the white noise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.arange(0, 10).float().unsqueeze(-1)\n",
    "\n",
    "pretty_print(y, \"y (response)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the case of linear regression, we usually wish to obtain a set of weights minimizing the so called mean square error/loss (MSE), which is the squared difference between the ground truth and the model prediction, summed for each data instance.\n",
    "\n",
    "We know that the OLS/Max Likelihood esitmator is the one yielding the optimal set of weights in that regard.\n",
    "\n",
    "$\\hat{W} = (X^\\top X)^{-1} X^\\top y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_hat = ((X.T @ X).inverse()) @ X.T @ y # OLS estimator\n",
    "\n",
    "pretty_print(W_hat, \"W (optimal weights/coefficients [first 3] and bias/intercept [last one])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate our model on the mean square loss\n",
    "\n",
    "**Q**: what is its formula?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_loss(y, y_hat):\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply it to our data.\n",
    "\n",
    "First we need to obtain the predictions, then we can evaluate the MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = X @ W_hat\n",
    "pretty_print(y_hat, \"Predictions (y_hat)\")\n",
    "\n",
    "pretty_print(mean_square_loss(y, y_hat), \"Loss (MSE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using PT built-ins\n",
    "\n",
    "We will now be exploring the second chunk of PT functionalities, namely the built-in structures and routines supporting the creation of ML models.\n",
    "\n",
    "We can create the same model we have seen before using PT built-in structures, so we start to see them right away.\n",
    "\n",
    "Usually, a PT model is a `class` inheriting from `torch.nn.Module`. Inside this class, we'll define two methods:\n",
    "* the constructor (`__init__`) in which we define the building blocks of our model as class variables (later during our lectures we'll see more \"elegant\" methods to build models architectures)\n",
    "* the `forward` method, which specifies how the data fed into the model needs to be processed in order to produce the output\n",
    "\n",
    "Note for those who already know something about NNs: we don't need to define `backward` methods since we're constructing our model with built-in PT building blocks. PT automatically creates a `backward` routine based upon the `forward` method.\n",
    "\n",
    "Our model only has one building block (layer) which is a `Linear` layer.\n",
    "We need to specify the size of the input (i.e. the coefficients `W` of our linear regressor) and the size of the output (i.e. how many scalars it produces) of the layer. We additionaly request our layer to have a bias term `b` (which acts as the intercept of the hyperplane we saw before).\n",
    "\n",
    "The `Linear` layer processes its input as `XW + b`, which is exactly the (first) equation of the linear regressor we saw before.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.regressor = torch.nn.Linear(in_features=3, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.regressor(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an instance of our model and inspect the current parameters by using the `state_dict` method, which prints the building blocks of our model and their current parameters. Note that `state_dict` is essentially a dictonary indexed by the names of the building blocks which we defined inside the constructor (plus some additional identifiers if a layer has more than one set of parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegressor()\n",
    "\n",
    "for param_name, param in lin_reg.state_dict().items():\n",
    "    print(param_name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can update the parameters via `state_dict` and re-using the same OLS estimates we obtained before.\n",
    "\n",
    "Note that PT is thought of for Deep Learning: it does not have (I think) the routines to solve different ML problems.\n",
    "\n",
    "Next time, we'll see how we can unleash PT's gradient-based iterative training routines and compare the results w.r.t. the OLS estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = lin_reg.state_dict()\n",
    "state_dict[\"regressor.weight\"] = W_hat[:3].T\n",
    "state_dict[\"regressor.bias\"] = W_hat[3]\n",
    "lin_reg.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if it worked\n",
    "for param_name, param in lin_reg.state_dict().items():\n",
    "    print(param_name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `forward` method gets implicitly called by passing the data to our model's instance `lin_reg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lin_reg = X[:,:3]\n",
    "predictions_lin_reg = lin_reg(X_lin_reg)\n",
    "pretty_print(predictions_lin_reg, \"Predictions of torch class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions are the same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(y_hat, \"Predictions of linear model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding non-linearity\n",
    "\n",
    "One of the staples of DL is that the relationship between the `X`s and the predictions is **non-linear**.\n",
    "\n",
    "The non-linearity is obtain by applying a non-linear function (called *activation function*) after each linear layer.\n",
    "\n",
    "We can complicate just a little bit our linear model to create a **logistic regressor**:\n",
    "\n",
    "$y = \\text{logistic}(XW + b)$,\n",
    "\n",
    "where $\\text{logistic}(z) = \\exp(z) / (1 + \\exp(z))$\n",
    "\n",
    "The logistic function has different names:\n",
    "* in statistics, it's usually called *inverse logit* as well\n",
    "* in DL and mathematics, it's called *sigmoid function* due to its \"S\" shape and is often denoted with the symbol $\\sigma$\n",
    "\n",
    "Hystorically, the sigmoid was between the first activation functions used in NNs.\n",
    "\n",
    "![](img/sigmoid.png)\n",
    "\n",
    "Logistic regression is usually used as a **binary classification model** instead of a regression model.\n",
    "In this setting, we suppose we have two destination classes to which we assign values 0 and 1: `y ∈ {0, 1}`.\n",
    "Since the codomain of the sigmoid is `[0,1]`, we can interpret its output `ŷ` as a probability value, and assign each data to the class 0 if `ŷ <= 0.5`, to the class 1 otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.Tensor([0,1,0,0,1,1,1,0,1,1])\n",
    "pretty_print(y, \"y for our classification problem\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we may also want our y to be a vector of `int`s.\n",
    "We can convert the `Tensor` type to `int` by calling the method `.long()` or `.int()` of `Tensor`.\n",
    "Usually, PT uses the long datatype for labels.\n",
    "\n",
    "As in NumPy, the type of the `Tensor` is found within the `dtype` variable of the given `Tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.long()\n",
    "pretty_print(y, \"y converted to int\")\n",
    "pretty_print(y.dtype, \"Data type of y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now build our logistic regressor in PT.\n",
    "\n",
    "We only need one single addition wrt the linear regressor: in the `forward` method, we'll add the sigmoidal non-linearity by calling the `sigmoid` function within the `torch.nn.functional` library.\n",
    "\n",
    "Note that there also exist some \"mirror\" alias of these functionals within `torch.nn` (e.g. `torch.nn.Sigmoid`): we'll learn in the following lecture why these aliases are there and how to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # no difference wrt linear regressor\n",
    "        self.regressor = torch.nn.Linear(in_features=3, out_features=1)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        out = self.regressor(X)\n",
    "        # here we apply the sigmoid fct to the output of regressor\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can instantiate our logistic regressor and use it to calculate our predictions on the same X as before.\n",
    "\n",
    "Note that **we're using the initial (random) weights which PT has assigned to the model parameters**.\n",
    "For our linear regressor, we were able to analytically obtain the OLS value of the parameters.\n",
    "In the case of logistic regression, there's no MaxLikelihood estimator obtainable in close form and we need to resort to numerical methods to obtain them.\n",
    "Since the part concerning numerical optimization will be discussed during the next Lab, we will not be training our model (hence results will obviously be sub-par)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegressor()\n",
    "y_hat = log_reg(X_lin_reg)\n",
    "pretty_print(y_hat, \"logistic regressor predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exist many ways to evaluate the performance of the logistic regressor: one of them is **accuracy** (correctly identified units / total number of units).\n",
    "We can define a function to evaluate accuracy and calculate it on our model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    # Assign each y_hat to its predicted class\n",
    "    pred_classes = torch.where(y_hat < .5, 0, 1).squeeze().long()\n",
    "    correct = (pred_classes == y).sum()\n",
    "    return (correct / y.shape[0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing linear and logistic regression as a computational graph\n",
    "\n",
    "We now need to convert the equation of the linear and the logistic regression:\n",
    "* `y = σ(WX + b)`\n",
    "\n",
    "where `σ` is a generic `ℝ → ℝ` function: sigmoid for logistic regression, identity for linear regression.\n",
    "\n",
    "![](imgs/01/log_reg_graph.jpg)\n",
    "\n",
    "We organize the input in *nodes* (on the left part) s.t. each node represents one dimension/covariate.\n",
    "For each data instance, we substitute to each node the corresponding numeric value.\n",
    "The nodes undergo one or more operations, namely, from left to right:\n",
    "\n",
    "1. Each node is multiplied by its corresponding weight (a value placed on the edge indicates that the node is multiplied by said value)\n",
    "2. All the corresponding outputs are summed together\n",
    "\n",
    "These two operations identify the dot product between vectors `X` and `W`\n",
    "\n",
    "3. The bias term `b` is added\n",
    "4. The non-linear function `σ` is applied to the result of this sum\n",
    "5. Finally, we assign that value to the variable `ŷ`, which is also indicated as a node\n",
    "\n",
    "**Q**: what is the relationship between $b$ and $X_4$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our first MultiLayer Perceptron (MLP)\n",
    "\n",
    "The MLP is a family of Artificial NNs in which the input is a vector of size `ℝ^d` and the output is again a vector of size `ℝ^p`, where `p` is determined upon the nature of the problem we wish to solve. Additionally, a MLP is characterized by multiple stages (*layers*) of sequential vector-matrix multiplication and non-linearity (steps 1., 2., 3. above) in which each output of the layer `l-1` acts as input to the layer `l`.\n",
    "\n",
    "Taking inspiration to the graph of the logistic regression, we can translate all into an image to give sense to these words:\n",
    "\n",
    "![](imgs/01/mlp_graph.jpg)\n",
    "\n",
    "In NNs, each of the nodes within the graph is called a **neuron**\n",
    "\n",
    "Neurons are organized in **layers**\n",
    "\n",
    "In computational graphs, layers are shown from left to right (or bottom to top sometimes), which is the direction of the flow of information inside the NN.\n",
    "\n",
    "The first layer is called **input layer** and represents the dimensions of our data.\n",
    "\n",
    "The last layer is called **output layer** and represent the output of our NN.\n",
    "\n",
    "All the intermediate layers are called **hidden layers**. To be defined MLP, there must be at least one hidden layer inside our model.\n",
    "\n",
    "If the NN is an MLP, each neuron in a given layer (except for the input) receives information from every neuron of the previous; moreover, each neuron in any layer (except for the output) sends information to every neuron of the next layer. There's no communication between neurons of the same layer (if it happens, we have a **Recursive Neural Network**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of brevity, usually in NN computational graphs we drop the blocks `+` and `σ`, the values of weights, and the reference to the bias terms, remaining with a scheme conveying info about\n",
    "* the number of neurons per layer\n",
    "* the connectivity of neurons\n",
    "\n",
    "The graph above becomes:\n",
    "\n",
    "![](imgs/01/mlp_graph_common.jpg)\n",
    "\n",
    "We can then start programming our simple MLP in PT.\n",
    "\n",
    "We will suppose that our MLP is for **binary classification**, hence the activation function `τ` is the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(in_features=3, out_features=2)\n",
    "        self.layer2 = torch.nn.Linear(in_features=2, out_features=1)\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.layer1(X)\n",
    "        out = torch.nn.functional.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        out = torch.nn.functional.sigmoid(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the great majority of MLP, it's very hard to get analytical solutions to our sets of weights and biases. We then resort to numerical methods for optimization.\n",
    "\n",
    "In DL, we normally used gradient-based methods like Stochastic Gradient Descent with *backpropagation* to find approximate solutions.\n",
    "\n",
    "We'll cover these topics in future lectures. For now, the focus is to build a MLP in PT and perform the *forward pass* (=evaluate the model on a set of data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can analyse the structure of our MLP by just printing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "although we might wanna have additional informations.\n",
    "\n",
    "There's an additional package, called `torchinfo` which helps us producing more informative and exhaustive model summaries.\n",
    "\n",
    "We can install it from the terminal:\n",
    "\n",
    "- activate the conda env\n",
    "- `conda install -c conda-forge torchinfo`\n",
    "\n",
    "On Colab, remember that you can execute bash commands prepending a `!` to the command itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usage of `Sequential`\n",
    "\n",
    "If we have a neural network whose forward pass just processes the layers in sequence in a given order, we can also wrap the layers inside a `torch.nn.Sequential` structure.\n",
    "\n",
    "Note that:\n",
    "1. In this case, all the activation functions must be passed as their `nn.Module` counterpart, not as `torch.nn.functional` functions\n",
    "2. The data flows in the layers according to the order dictated in the constructor\n",
    "\n",
    "**Q**: How can I build the `forward` method now?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(3, 2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(2, 1),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        # how can I complete it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us suppose we wish to build a larger model from the graph below.\n",
    "\n",
    "![](imgs/01/mlp_graph_larger.jpg)\n",
    "\n",
    "We suppose that\n",
    "\n",
    "1. The layers have no bias units\n",
    "2. The activation function for hidden layers is `ReLU`\n",
    "\n",
    "    $ \\text{ReLU}(x) = \\max(0, x)$\n",
    "\n",
    "Moreover, we suppose that this is a classification problem.\n",
    "\n",
    "As you might recall, when the number of classes is > 2, we encode the problem in such a way that the output layer has a no. of neurons corresponding to the no. of classes. Doing so, we establish a correspondence between output units and classes. The value of the $j$-th neuron represents the **confidence** of the network in assigning a given data instance to the $j$-th class.\n",
    "\n",
    "Classically, when the network is encoded in such way, the activation function for the final layer is the **softmax** function.\n",
    "If $C$ is the total number of classes,\n",
    "\n",
    "$softmax(z_j) = \\frac{\\exp(z_j)}{\\sum_{k=1}^C \\exp(z_k)}$\n",
    "\n",
    "where $j\\in \\{1,\\cdots,C\\}$ is one of the classes.\n",
    "\n",
    "If we repeat this calculation for all $j$ s, we end up with $C$ normalized values (i.e., between 0 and 1) which can be interpreted as a confidence that the network has in assigning the instance to the corresponding class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homework\n",
    "\n",
    "1. build the MLP in the image above using PT built-ins\n",
    "2. Provide calculation for the exact number of parameters of the MLP\n",
    "   - Do it first supposing that the layers don't have a bias term, then supposing that the bias is present wherever it's possible\n",
    "3. Calculate the $L_1$ (vectorial) norm and the Frobenius norm for the params of each layer\n",
    "4. Given 10 random datapoints, feed them into the network. This operation must be done all in one single command and must **not** make use of loops.\n",
    "   - Given the output of the network, using PyTorch code, find the class of assignment of each datapoint. This also must be done in a single PyTorch command without using loops.\n",
    "   - Drafting a vector of ground truths (whichever labels you like), provide code for the calculation of the accuracy\n",
    "     - Tip: first get the number of correct assignments, then..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
